{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision.models as torchmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the wandb version\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘† check not version 0.13.10 as this falls over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kgoldmann/Documents/Projects/AMBER/on_device_classifier/02_model_training/pytorch\n"
     ]
    }
   ],
   "source": [
    "# get wd\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "print(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.build_model import build_model\n",
    "from data2.mothdataset import MOTHDataset\n",
    "from training_params.loss import Loss\n",
    "from training_params.optimizer import Optimizer\n",
    "from data2 import dataloader\n",
    "from evaluation.micro_accuracy_batch import MicroAccuracyBatch\n",
    "from evaluation.micro_accuracy_batch import add_batch_microacc, final_microacc\n",
    "from evaluation.macro_accuracy_batch import MacroAccuracyBatch\n",
    "from evaluation.macro_accuracy_batch import add_batch_macroacc, final_macroacc, taxon_accuracy\n",
    "from evaluation.confusion_matrix_data import confusion_matrix_data\n",
    "from evaluation.confusion_data_conversion import ConfusionDataConvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = './configs/01_uk_moth_data_config.json'\n",
    "dataloader_num_workers = 4\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f             = open(config_file)\n",
    "config_data   = json.load(f)\n",
    "print(json.dumps(config_data, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data['training']['wandb']['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=config_data['training']['wandb']['project'], entity=config_data['training']['wandb']['entity'])\n",
    "wandb.init(settings=wandb.Settings(start_method=\"fork\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "image_resize  = config_data['training']['image_resize']\n",
    "batch_size    = config_data['training']['batch_size']\n",
    "label_list    = config_data['dataset']['label_info']\n",
    "epochs        = config_data['training']['epochs']\n",
    "loss_name     = config_data['training']['loss']['name']\n",
    "early_stop    = config_data['training']['early_stopping']\n",
    "start_val_los = config_data['training']['start_val_loss']\n",
    "\n",
    "label_read    = json.load(open(label_list))\n",
    "species_list  = label_read['species_list']\n",
    "genus_list    = label_read['genus_list']\n",
    "family_list   = label_read['family_list']\n",
    "\n",
    "no_species_cl = config_data['model']['species_num_classes']\n",
    "no_genus_cl   = config_data['model']['genus_num_classes']\n",
    "no_family_cl  = config_data['model']['family_num_classes']\n",
    "model_type    = config_data['model']['type']\n",
    "preprocess_mode = config_data['model']['preprocess_mode']\n",
    "\n",
    "opt_name      = config_data['training']['optimizer']['name']\n",
    "learning_rate = config_data['training']['optimizer']['learning_rate']\n",
    "momentum      = config_data['training']['optimizer']['momentum']\n",
    "\n",
    "mod_save_pth  = config_data['training']['model_save_path']\n",
    "mod_name      = config_data['training']['model_name']\n",
    "mod_ver       = config_data['training']['version']\n",
    "DTSTR         = datetime.datetime.now()\n",
    "DTSTR         = DTSTR.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_path     = mod_save_pth + mod_name + '_' + mod_ver + '_' + model_type + '_' + DTSTR + '.pt'\n",
    "\n",
    "taxon_hierar  = config_data['dataset']['taxon_hierarchy']\n",
    "label_info    = config_data['dataset']['label_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model\n",
    "# Get cpu or gpu device for training.\n",
    "if (torch.cuda.is_available()) and (not torch.backends.mps.is_available()):\n",
    "\tdevice = \"cuda\" \n",
    "elif torch.backends.mps.is_available():\n",
    "\tdevice = \"mps\"\n",
    "else: \n",
    "\tdevice =\"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config_data)\n",
    "\n",
    "# Making use of multiple GPUs\n",
    "if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "\tprint(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\tmodel = nn.DataParallel(model)\n",
    "\n",
    "if device == \"mps\" and torch.cuda.device_count() > 1:\n",
    "\tprint(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\tmodel = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_webdataset_url = \"./data2/datasets/test/train/train-500-{000000..000002}.tar\"\n",
    "val_webdataset_url = \"./data2/datasets/test/val/val-500-000000.tar\"\n",
    "test_webdataset_url = \"./data2/datasets/test/test/test-500-000000.tar\"\n",
    "\n",
    "dataloader_num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading Data\n",
    "# # Training data loader\n",
    "train_dataloader = dataloader.build_webdataset_pipeline(\n",
    "\tsharedurl=train_webdataset_url,\n",
    "\tinput_size=image_resize,\n",
    "\tbatch_size=batch_size,\n",
    "\tis_training=True,\n",
    "\tnum_workers=dataloader_num_workers,\n",
    "\tpreprocess_mode=preprocess_mode)\n",
    "\n",
    "\n",
    "# Validation data loader\n",
    "val_dataloader = dataloader.build_webdataset_pipeline(\n",
    "\tsharedurl=val_webdataset_url,\n",
    "\tinput_size=image_resize,\n",
    "\tbatch_size=batch_size,\n",
    "\tis_training=False,\n",
    "\tnum_workers=dataloader_num_workers,\n",
    "\tpreprocess_mode=preprocess_mode)\n",
    "\n",
    "# Testing data loader\n",
    "test_dataloader = dataloader.build_webdataset_pipeline(\n",
    "\tsharedurl=test_webdataset_url,\n",
    "\tinput_size=image_resize,\n",
    "\tbatch_size=batch_size,\n",
    "\tis_training=False,\n",
    "\tnum_workers=dataloader_num_workers,\n",
    "\tpreprocess_mode=preprocess_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Loss function and Optimizer\n",
    "loss_func = Loss(loss_name).func()\n",
    "optimizer = Optimizer(opt_name, model, learning_rate, momentum).func()\n",
    "\n",
    "# Model Training\n",
    "lowest_val_loss = start_val_los\n",
    "early_stp_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image_batch in train_dataloader\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "#image_batch.shape, label_batch.shape\n",
    "\n",
    "image_batch, label_batch = image_batch.to(device, non_blocking=True), label_batch.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "timm.create_model('tf_efficientnetv2_b3', pretrained=True, num_classes=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below takes 1-2 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from evaluation.confusion_matrix_data import confusion_matrix_data\n",
    "from evaluation.confusion_data_conversion import ConfusionDataConvert\n",
    "from evaluation.micro_accuracy_batch import MicroAccuracyBatch\n",
    "from evaluation.micro_accuracy_batch import add_batch_microacc, final_microacc\n",
    "from evaluation.macro_accuracy_batch import MacroAccuracyBatch\n",
    "from evaluation.macro_accuracy_batch import add_batch_macroacc, final_macroacc, taxon_accuracy\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\ttrain_loss      = 0\n",
    "\ttrain_batch_cnt = 0\n",
    "\tval_loss        = 0\n",
    "\tval_batch_cnt   = 0\n",
    "\ts_time          = time.time()\n",
    "\t\n",
    "\tglobal_microacc_data_train = None\n",
    "\tglobal_microacc_data_val   = None\n",
    "\t\n",
    "\t# model training on training dataset\n",
    "\tmodel.train()                      \n",
    "\tfor image_batch, label_batch in train_dataloader: \n",
    "\t\t#print(image_batch)\n",
    "\t\t#print(label_batch)   \n",
    "\t\t#print(image_batch.to(device, non_blocking=True))\n",
    "\t\timage_batch,  label_batch = image_batch.to(device, non_blocking=True), label_batch.to(device, non_blocking=True)          \n",
    "\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# forward + backward + optimize\n",
    "\t\toutputs   = model(image_batch)      \n",
    "\t\tt_loss    = loss_func(outputs, label_batch)\n",
    "\t\tt_loss.backward()\n",
    "\t\toptimizer.step()        \n",
    "\t\ttrain_loss += t_loss.item()\n",
    "\t\t\n",
    "\t\t# micro-accuracy calculation\n",
    "\t\tmicro_accuracy_train          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "\t\tglobal_microacc_data_train    = add_batch_microacc(global_microacc_data_train, micro_accuracy_train)\n",
    "\t\ttrain_batch_cnt += 1\n",
    "\ttrain_loss = train_loss/train_batch_cnt\n",
    "\n",
    "\n",
    "\t# model evaluation on validation dataset\n",
    "\tmodel.eval()                      \n",
    "\tfor image_batch, label_batch in val_dataloader:\n",
    "\t\timage_batch, label_batch = image_batch.to(device, non_blocking=True), label_batch.to(device, non_blocking=True)        \n",
    "\t\n",
    "\t\toutputs   = model(image_batch)        \n",
    "\t\tv_loss    = loss_func(outputs, label_batch)\n",
    "\t\tval_loss += v_loss.item()    \n",
    "\t\n",
    "\t\t# micro-accuracy calculation\n",
    "\t\tmicro_accuracy_val          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "\t\tglobal_microacc_data_val    = add_batch_microacc(global_microacc_data_val, micro_accuracy_val)\n",
    "\t\tval_batch_cnt += 1\n",
    "\tval_loss = val_loss/val_batch_cnt\n",
    "\n",
    "\tif val_loss<lowest_val_loss:\n",
    "\t\tif torch.cuda.device_count() > 1:\n",
    "\t\t\ttorch.save({\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': model.module.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t'val_loss':val_loss}, \n",
    "\t\t\t\tsave_path)   \n",
    "\t\telse:\n",
    "\t\t\ttorch.save({\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t'val_loss':val_loss}, \n",
    "\t\t\t\tsave_path)    \n",
    "\t\t\t\n",
    "\t\tlowest_val_loss = val_loss\n",
    "\t\tearly_stp_count = 0\n",
    "\telse:\n",
    "\t\tearly_stp_count += 1\n",
    "\n",
    "\t# logging metrics\n",
    "\twandb.log({'training loss': train_loss, 'validation loss': val_loss, 'epoch': epoch})\n",
    "\n",
    "\tfinal_micro_accuracy_train = final_microacc(global_microacc_data_train)\n",
    "\tfinal_micro_accuracy_val   = final_microacc(global_microacc_data_val) \n",
    "\twandb.log({'train_micro_species_top1': final_micro_accuracy_train['micro_species_top1'], \n",
    "\t\t\t'train_micro_genus_top1': final_micro_accuracy_train['micro_genus_top1'],\n",
    "\t\t\t'train_micro_family_top1': final_micro_accuracy_train['micro_family_top1'],\n",
    "\t\t\t'val_micro_species_top1': final_micro_accuracy_val['micro_species_top1'], \n",
    "\t\t\t'val_micro_genus_top1': final_micro_accuracy_val['micro_genus_top1'],\n",
    "\t\t\t'val_micro_family_top1': final_micro_accuracy_val['micro_family_top1'],\n",
    "\t\t\t'epoch': epoch   \n",
    "\t\t\t})   \n",
    "\n",
    "\te_time = (time.time()-s_time)/60   # time taken in minutes    \n",
    "\twandb.log({'time per epoch': e_time, 'epoch': epoch})\n",
    "\n",
    "\tif early_stp_count >= early_stop:\n",
    "\t\tbreak    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log_artifact(save_path, name=mod_name, type='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()                                          \n",
    "global_microacc_data     = None\n",
    "global_macroacc_data     = None\n",
    "global_confusion_data_sp = None\n",
    "global_confusion_data_g  = None\n",
    "global_confusion_data_f  = None\n",
    "\n",
    "print(\"Prediction on test data started ...\")\n",
    "\n",
    "with torch.no_grad():                                 \n",
    "\tfor image_batch, label_batch in test_dataloader:  \n",
    "\t\timage_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "\t\tpredictions              = model(image_batch)\n",
    "\t\n",
    "\t\t# micro-accuracy calculation\n",
    "\t\tmicro_accuracy           = MicroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "\t\tglobal_microacc_data     = add_batch_microacc(global_microacc_data, micro_accuracy)\n",
    "\t\n",
    "\t\t# macro-accuracy calculation\n",
    "\t\tmacro_accuracy           = MacroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()\n",
    "\t\tglobal_macroacc_data     = add_batch_macroacc(global_macroacc_data, macro_accuracy) \n",
    "\n",
    "\t\t# confusion matrix\n",
    "\t\tsp_label_batch, sp_predictions, g_label_batch, g_predictions, f_label_batch, f_predictions = ConfusionDataConvert(predictions, label_batch, label_info, taxon_hierar).converted_data()   \n",
    "\t\n",
    "\t\tglobal_confusion_data_sp = confusion_matrix_data(global_confusion_data_sp, [sp_label_batch, sp_predictions])\n",
    "\t\tglobal_confusion_data_g  = confusion_matrix_data(global_confusion_data_g, [g_label_batch, g_predictions])\n",
    "\t\tglobal_confusion_data_f  = confusion_matrix_data(global_confusion_data_f, [f_label_batch, f_predictions])        \n",
    "\n",
    "final_micro_accuracy            = final_microacc(global_microacc_data)\n",
    "final_macro_accuracy, taxon_acc = final_macroacc(global_macroacc_data)\n",
    "tax_accuracy                    = taxon_accuracy(taxon_acc, label_read)\n",
    "\n",
    "# saving evaluation data to file\n",
    "confdata_pd_f  = pd.DataFrame({'F_Truth': global_confusion_data_f[0].reshape(-1), 'F_Prediction': global_confusion_data_f[1].reshape(-1)})\n",
    "confdata_pd_g  = pd.DataFrame({'G_Truth': global_confusion_data_g[0].reshape(-1), 'G_Prediction': global_confusion_data_g[1].reshape(-1)})\n",
    "confdata_pd_sp = pd.DataFrame({'S_Truth': global_confusion_data_sp[0].reshape(-1), 'S_Prediction': global_confusion_data_sp[1].reshape(-1)})\n",
    "confdata_pd    = pd.concat([confdata_pd_f, confdata_pd_g, confdata_pd_sp], axis=1)\n",
    "confdata_pd.to_csv(mod_save_pth + mod_ver + '_confusion-data.csv', index=False)\n",
    "\n",
    "with open(mod_save_pth + mod_name + '_' + mod_ver + '_micro-accuracy.json', 'w') as outfile:\n",
    "\tjson.dump(final_micro_accuracy, outfile)\n",
    "\n",
    "with open(mod_save_pth + mod_name + '_' + mod_ver + '_macro-accuracy.json', 'w') as outfile:\n",
    "\tjson.dump(final_macro_accuracy, outfile)\n",
    "\n",
    "with open(mod_save_pth + mod_name + '_' + mod_ver + '_taxon-accuracy.json', 'w') as outfile:\n",
    "\tjson.dump(tax_accuracy, outfile)\n",
    "\n",
    "wandb.log({'final micro accuracy' : final_micro_accuracy})\n",
    "wandb.log({'final macro accuracy' : final_macro_accuracy})\n",
    "wandb.log({'configuration' : config_data})\n",
    "wandb.log({'tax accuracy' : tax_accuracy})\n",
    "\n",
    "label_f = tf.keras.utils.to_categorical(global_confusion_data_f[0], num_classes=no_family_cl)\n",
    "pred_f  = tf.keras.utils.to_categorical(global_confusion_data_f[1], num_classes=no_family_cl)\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbif_species_trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
